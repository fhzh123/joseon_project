{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import deque, Counter\n",
    "\n",
    "import torch\n",
    "\n",
    "from dynamic_bernoulli_embeddings.training import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='DWE argparser')\n",
    "# Path Setting\n",
    "parser.add_argument('--data_path', type=str, default='../joseon_word_embedding/data/', \n",
    "                    help='Data path setting')\n",
    "parser.add_argument('--save_path', type=str, default='./save2',\n",
    "                    help='Save path setting')\n",
    "# Training Setting\n",
    "parser.add_argument('--num_epochs', type=int, default=5, help='The number of epoch')\n",
    "args = parser.parse_args(list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:25<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data List Setting\n",
    "data_path = glob(os.path.join(args.data_path, '*.json'))\n",
    "data_path = sorted(data_path)[:-1] # 순종부록 제거\n",
    "\n",
    "# Preprocessing\n",
    "total_counter = Counter()\n",
    "king_list = list()\n",
    "king_index_list = list()\n",
    "comment_list = list()\n",
    "\n",
    "# start_time = time.time()\n",
    "for ix, path in enumerate(tqdm(data_path)):\n",
    "    with open(path, 'r') as f:\n",
    "        record_list = json.load(f)\n",
    "        king_list.append(path.split(' ')[-1][:2])\n",
    "        king_index_list.append(ix)\n",
    "        total_record = list()\n",
    "\n",
    "        for rc in record_list:\n",
    "            total_record.append(rc['hanja'])\n",
    "        total_record = ' '.join(total_record)\n",
    "        new_word = re.sub(pattern='[^\\w\\s]', repl='', string=total_record)\n",
    "        new_word = re.sub(pattern='([ㄱ-ㅎㅏ-ㅣ]+)', repl='', string=new_word)\n",
    "        new_word = re.sub(pattern='[\\u3131-\\u3163\\uac00-\\ud7a3]+', repl='', string=new_word)\n",
    "        new_word = re.sub(pattern='[a-zA-Z0-9]+', repl='', string=new_word)\n",
    "        comment_list.append(new_word)\n",
    "        total_counter.update(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(total_counter.keys())\n",
    "vocab.insert(0, '<unk>')\n",
    "vocab.insert(0, '</s>')\n",
    "vocab.insert(0, '<s>')\n",
    "vocab.insert(0, '<pad>')\n",
    "word2id = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "# Dataset Setting\n",
    "dataset = pd.DataFrame({\n",
    "    'session': king_list,\n",
    "    'text': comment_list,\n",
    "    'time': king_index_list\n",
    "})\n",
    "dataset['bow'] = dataset['text'].apply(lambda x: [i for i in x])\n",
    "\n",
    "# Generate dictionary.\n",
    "dictionary = Dictionary(dataset.bow)\n",
    "dictionary.filter_extremes(no_below=15, no_above=1.)\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from dynamic_bernoulli_embeddings.embeddings import DynamicBernoulliEmbeddingModel\n",
    "from dynamic_bernoulli_embeddings.preprocessing import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = None\n",
    "dictionary = word2id\n",
    "num_epochs = args.num_epochs\n",
    "notebook = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for gpu.\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create a validation set.\n",
    "validation_mask = np.repeat(False, dataset.shape[0])\n",
    "if validation is not None:\n",
    "    assert 0 < validation < 1\n",
    "    validation_mask = np.random.random(dataset.shape[0]) < validation\n",
    "data = Data(dataset[~validation_mask], dictionary, device)\n",
    "data_val = Data(dataset[validation_mask], dictionary, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains embedding model implementation\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "class DynamicBernoulliEmbeddingModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        V,\n",
    "        T,\n",
    "        m_t,\n",
    "        dictionary,\n",
    "        sampling_distribution,\n",
    "        k=256,\n",
    "        lambda_=1e4,\n",
    "        lambda_0=1,\n",
    "        ns=20,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        V : int\n",
    "            Vocabulary size.\n",
    "        T : int\n",
    "            Number of timesteps.\n",
    "        m_t : dict\n",
    "            The total number of tokens in each timestep to compute the scaling factor\n",
    "            for the pseudo log likelihoods.\n",
    "        dictionary : dict\n",
    "            Maps word to index.\n",
    "        sampling_distribution : tensor (V,)\n",
    "            The unigram distribution to use for negative sampling.\n",
    "        k : int\n",
    "            Embedding dimension.\n",
    "        lambda_ : int\n",
    "            Scaling factor on the time drift prior.\n",
    "        lambda_0 : int\n",
    "            Scaling factor on the embedding priors.\n",
    "        ns : int\n",
    "            Number of negative samples.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.V = V  # Vocab size.\n",
    "        self.T = T  # Number of timestepss.\n",
    "        self.k = k  # Embedding dimension.\n",
    "        self.total_tokens = sum(m_t.values())  # Used for scaling factor for pseudo LL\n",
    "        self.lambda_ = lambda_  # Scaling factor on the time drift prior.\n",
    "        self.lambda_0 = lambda_0  # Scaling factor on the embedding priors.\n",
    "        self.sampling_distribution = Categorical(logits=sampling_distribution)\n",
    "        self.negative_samples = ns  # Number of negative samples.\n",
    "        self.dictionary = dictionary\n",
    "        self.dictionary_reverse = {v: k for k, v in dictionary.items()}\n",
    "\n",
    "        # Embeddings parameters.\n",
    "        self.rho = nn.Embedding(V * T, 50)  # Stacked dynamic embeddings # 이부분\n",
    "        self.rho2 = nn.Linear(50, k)\n",
    "        self.alpha = nn.Embedding(V, 50)  # Time independent context embeddings # 이부분\n",
    "        self.alpha2 = nn.Linear(50, k)\n",
    "        with torch.no_grad():\n",
    "            nn.init.normal_(self.rho.weight, 0, 0.01)\n",
    "            nn.init.normal_(self.alpha.weight, 0, 0.01)\n",
    "\n",
    "        # Transformations\n",
    "        self.log_sigmoid = nn.LogSigmoid()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def L_pos(self, eta):\n",
    "        return self.log_sigmoid(eta).sum()\n",
    "\n",
    "    def L_neg(self, batch_size, times, contexts_summed):\n",
    "        neg_samples = self.sampling_distribution.sample(\n",
    "            torch.Size([batch_size, self.negative_samples])\n",
    "        )\n",
    "        neg_samples = neg_samples + (times * self.V).reshape((-1, 1))\n",
    "        neg_samples = neg_samples.T.flatten() # 이부분\n",
    "        context_flat = contexts_summed.repeat((self.negative_samples, 1))\n",
    "        testing = self.rho(neg_samples)\n",
    "        eta_neg = (self.rho2(self.rho(neg_samples)) * context_flat).sum(axis=1)\n",
    "        return (torch.log(1 - self.sigmoid(eta_neg) + 1e-7)).sum()\n",
    "\n",
    "    def forward(self, targets, times, contexts, validate=False, dynamic=True):\n",
    "        \"\"\"Forward pass of the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        targets : (batch_size,)\n",
    "        times : (batch_size,)\n",
    "        contexts : (batch_size, 2 * context_size)\n",
    "        dynamic : bool\n",
    "            Indicates whether to include the drift component of the loss.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss\n",
    "        L_pos\n",
    "        L_neg\n",
    "        L_prior\n",
    "        \"\"\"\n",
    "        batch_size = targets.shape[0]\n",
    "        print(f'Dynamic : {dynamic}')\n",
    "\n",
    "        # Since the embeddings are stacked, adjust the indices for the targets.\n",
    "        # In other words, word `i` in time slice `j` would be at position\n",
    "        # `j * V + i` in the embedding matrix where V is the vocab size.\n",
    "        targets_adjusted = times * self.V + targets\n",
    "\n",
    "        # -1 indicates out of bounds for the context word, so mask these out so\n",
    "        # they don't contribute to the context sum.\n",
    "        context_mask = contexts == -1\n",
    "        contexts[context_mask] = 0\n",
    "        contexts = self.alpha2(self.alpha(contexts))\n",
    "        contexts[context_mask] = 0\n",
    "        contexts_summed = contexts.sum(axis=1)\n",
    "        print(self.rho(targets_adjusted).size())\n",
    "        print(self.rho2(self.rho(targets_adjusted)))\n",
    "        eta = (self.rho2(self.rho(targets_adjusted)) * contexts_summed).sum(axis=1)\n",
    "\n",
    "        # Loss\n",
    "        loss, L_pos, L_neg, L_prior = None, None, None, None\n",
    "\n",
    "        L_pos = self.L_pos(eta)\n",
    "        if not validate:\n",
    "            L_neg = self.L_neg(batch_size, times, contexts_summed)\n",
    "            loss = (self.total_tokens / batch_size) * (L_pos + L_neg)\n",
    "            L_prior = -self.lambda_0 / 2 * (self.alpha.weight ** 2).sum()\n",
    "            L_prior += -self.lambda_0 / 2 * (self.alpha2.weight ** 2).sum()\n",
    "            L_prior += -self.lambda_0 / 2 * (self.rho.weight[0] ** 2).sum()\n",
    "            L_prior += -self.lambda_0 / 2 * (self.rho2.weight[0] ** 2).sum()\n",
    "            if dynamic:\n",
    "                print('Dynamic!!')\n",
    "                print(self.T)\n",
    "                print(self.V)\n",
    "                rho_trans = self.rho.weight.reshape((self.T, self.V, 50))\n",
    "                print(rho_trans.size())\n",
    "                print(f'Weight: {self.rho2.weight.size()}')\n",
    "                rho_trans2 = self.rho2.weight.reshape((self.T, self.V, self.k))\n",
    "                print(rho_trans2.size())\n",
    "                L_prior += (\n",
    "                    -self.lambda_ / 2 * ((rho_trans[1:] - rho_trans[:-1]) ** 2).sum()\n",
    "                )\n",
    "                L_prior += (\n",
    "                    -self.lambda_ / 2 * ((rho_trans2[1:] - rho_trans2[:-1]) ** 2).sum()\n",
    "                )\n",
    "            loss += L_prior\n",
    "            loss = -loss\n",
    "\n",
    "        return loss, L_pos, L_neg, L_prior\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        \"\"\"Gets trained embeddings and reshapes them into (T, V, k)\"\"\"\n",
    "        embeddings = (\n",
    "            self.rho.cpu()\n",
    "            .weight.data.reshape((self.T, len(self.dictionary), self.k))\n",
    "            .numpy()\n",
    "        )\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model.\n",
    "model = DynamicBernoulliEmbeddingModel(\n",
    "    len(data.dictionary),\n",
    "    data.T,\n",
    "    data.m_t,\n",
    "    dictionary,\n",
    "    data.unigram_logits\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 300\n",
    "i = 1\n",
    "pbar = tqdm(enumerate(data.epoch(m)), total=m)\n",
    "pbar.set_description(f\"Epoch {i}\")\n",
    "for j, (targets, contexts, times) in pbar:\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    # The first epoch ignores time for initializing weights.\n",
    "    if i == 1:\n",
    "        times = torch.zeros_like(times)\n",
    "    loss, L_pos, L_neg, L_prior = model(targets, times, contexts, dynamic=i > 0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12800 / 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1350 / 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bow = list()\n",
    "for b in dataset['bow']:\n",
    "    total_bow.extend(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(total_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "55445088/184833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "184833 * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
