{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Module\n",
    "import os\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as torch_utils\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import Custom Module\n",
    "from translation.dataset import CustomDataset, PadCollate\n",
    "from translation.model import Transformer\n",
    "from translation.optimizer import Ralamb, WarmupLinearSchedule\n",
    "from translation.rnn_model import Encoder, Decoder, Seq2Seq\n",
    "from named_entity_recognition.model import NER_model\n",
    "from utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='NMT argparser')\n",
    "parser.add_argument('--save_path', default='./save2', \n",
    "                    type=str, help='path of data pickle file (train)')\n",
    "parser.add_argument('--resume', action='store_true',\n",
    "                    help='If not store, then training from scratch')\n",
    "parser.add_argument('--baseline', action='store_true',\n",
    "                    help='If not store, then training from Dynamic Word Embedding')\n",
    "parser.add_argument('--model_setting', default='rnn', choices=['transformer', 'rnn'],\n",
    "                    type=str, help='Model Selection; transformer vs rnn')\n",
    "parser.add_argument('--pad_idx', default=0, type=int, help='pad index')\n",
    "parser.add_argument('--bos_idx', default=1, type=int, help='index of bos token')\n",
    "parser.add_argument('--eos_idx', default=2, type=int, help='index of eos token')\n",
    "parser.add_argument('--unk_idx', default=3, type=int, help='index of unk token')\n",
    "\n",
    "parser.add_argument('--min_len', type=int, default=4, help='Minimum Length of Sentences; Default is 4')\n",
    "parser.add_argument('--max_len', type=int, default=500, help='Max Length of Source Sentence; Default is 150')\n",
    "\n",
    "parser.add_argument('--num_epoch', type=int, default=10, help='Epoch count; Default is 10')\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='Batch size; Default is 48')\n",
    "parser.add_argument('--lr', type=float, default=5e-5, help='Learning rate; Default is 5e-5')\n",
    "parser.add_argument('--lr_decay', type=float, default=0.5, help='Learning rate decay; Default is 0.5')\n",
    "parser.add_argument('--lr_decay_step', type=int, default=2, help='Learning rate decay step; Default is 5')\n",
    "parser.add_argument('--grad_clip', type=int, default=5, help='Set gradient clipping; Default is 5')\n",
    "parser.add_argument('--w_decay', type=float, default=1e-6, help='Weight decay; Default is 1e-6')\n",
    "\n",
    "parser.add_argument('--d_model', type=int, default=512, help='Hidden State Vector Dimension; Default is 512')\n",
    "parser.add_argument('--d_embedding', type=int, default=256, help='Embedding Vector Dimension; Default is 256')\n",
    "parser.add_argument('--n_head', type=int, default=8, help='Multihead Count; Default is 256')\n",
    "parser.add_argument('--dim_feedforward', type=int, default=512, help='Embedding Vector Dimension; Default is 512')\n",
    "parser.add_argument('--num_encoder_layer', default=8, type=int, help='number of encoder layer')\n",
    "parser.add_argument('--num_decoder_layer', default=8, type=int, help='number of decoder layer')\n",
    "parser.add_argument('--dropout', type=float, default=0.3, help='Dropout Ratio; Default is 0.5')\n",
    "\n",
    "parser.add_argument('--print_freq', type=int, default=300, help='Print train loss frequency; Default is 100')\n",
    "args = parser.parse_args(list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load & Setting!\n",
      "Build model\n",
      "Total Parameters: 102557376\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#===================================#\n",
    "#============Data Load==============#\n",
    "#===================================#\n",
    "\n",
    "print('Data Load & Setting!')\n",
    "with open(os.path.join(args.save_path, 'nmt_processed.pkl'), 'rb') as f:\n",
    "    data_ = pickle.load(f)\n",
    "    hj_train_indices = data_['hj_train_indices']\n",
    "    hj_test_indices = data_['hj_test_indices']\n",
    "    kr_train_indices = data_['kr_train_indices']\n",
    "    kr_test_indices = data_['kr_test_indices']\n",
    "    king_train_indices = data_['king_train_indices']\n",
    "    king_test_indices = data_['king_test_indices']\n",
    "    hj_word2id = data_['hj_word2id']\n",
    "    hj_id2word = data_['hj_id2word']\n",
    "    kr_word2id = data_['kr_word2id']\n",
    "    kr_id2word = data_['kr_id2word']\n",
    "    src_vocab_num = len(hj_word2id.keys())\n",
    "    trg_vocab_num = len(kr_word2id.keys())\n",
    "    del data_\n",
    "\n",
    "#===================================#\n",
    "#========DataLoader Setting=========#\n",
    "#===================================#\n",
    "\n",
    "dataset_dict = {\n",
    "    'train': CustomDataset(hj_train_indices, kr_train_indices, king_train_indices,\n",
    "                        min_len=args.min_len, max_len=args.max_len),\n",
    "    'valid': CustomDataset(hj_test_indices, kr_test_indices, king_test_indices,\n",
    "                        min_len=args.min_len, max_len=args.max_len)\n",
    "}\n",
    "dataloader_dict = {\n",
    "    'train': DataLoader(dataset_dict['train'], collate_fn=PadCollate(), drop_last=True,\n",
    "                        batch_size=args.batch_size, shuffle=True, pin_memory=True),\n",
    "    'valid': DataLoader(dataset_dict['valid'], collate_fn=PadCollate(), drop_last=True,\n",
    "                        batch_size=args.batch_size, shuffle=True, pin_memory=True)\n",
    "}\n",
    "\n",
    "#====================================#\n",
    "#==========DWE Results Open==========#\n",
    "#====================================#\n",
    "\n",
    "with open(os.path.join(args.save_path, 'emb_mat.pkl'), 'rb') as f:\n",
    "    emb_mat = pickle.load(f)\n",
    "\n",
    "#===================================#\n",
    "#===========Model Setting===========#\n",
    "#===================================#\n",
    "\n",
    "print(\"Build model\")\n",
    "if args.model_setting == 'transformer':\n",
    "    model = Transformer(emb_mat, hj_word2id, src_vocab_num, trg_vocab_num, pad_idx=args.pad_idx, bos_idx=args.bos_idx, \n",
    "                eos_idx=args.eos_idx, max_len=args.max_len,\n",
    "                d_model=args.d_model, d_embedding=args.d_embedding, n_head=args.n_head, \n",
    "                dim_feedforward=args.dim_feedforward, dropout=args.dropout,\n",
    "                num_encoder_layer=args.num_encoder_layer, num_decoder_layer=args.num_decoder_layer,\n",
    "                baseline=args.baseline, device=device)\n",
    "elif args.model_setting == 'rnn':\n",
    "    encoder = Encoder(src_vocab_num, args.d_embedding, args.d_model, \n",
    "                    emb_mat, hj_word2id, n_layers=args.num_encoder_layer, \n",
    "                    pad_idx=args.pad_idx, dropout=args.dropout)\n",
    "    decoder = Decoder(args.d_embedding, args.d_model, trg_vocab_num, n_layers=args.num_decoder_layer, \n",
    "                    pad_idx=args.pad_idx, dropout=args.dropout)\n",
    "    model = Seq2Seq(encoder, decoder, device)\n",
    "else:\n",
    "    raise Exception('Model error')\n",
    "\n",
    "if args.resume:\n",
    "    model_ner = NER_model(emb_mat=emb_mat, word2id=hj_word2id, pad_idx=args.pad_idx, bos_idx=args.bos_idx, eos_idx=args.eos_idx, max_len=args.max_len,\n",
    "                    d_model=args.d_model, d_embedding=args.d_embedding, n_head=args.n_head,\n",
    "                    dim_feedforward=args.dim_feedforward, n_layers=args.num_encoder_layer, dropout=args.dropout,\n",
    "                    crf_loss=args.crf_loss, device=device)\n",
    "    model_ner.load_state_dict(torch.load(os.path.join(args.save_path, 'ner_model_False.pt')))\n",
    "    model.transformer_encoder.load_state_dict(model_ner.transformer_encoder.state_dict())\n",
    "    for param in model.transformer_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "print(\"Total Parameters:\", sum([p.nelement() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=args.w_decay)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_decay_step, gamma=args.lr_decay)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=len(dataloader_dict['train'])*3, \n",
    "                                 t_total=len(dataloader_dict['train'])*args.num_epoch)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=args.pad_idx)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/17201 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitting: [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 143/17201 [04:53<9:43:49,  2.05s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c908ba871faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# If phase train, then backward loss and step optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "total_train_loss_list = list()\n",
    "total_test_loss_list = list()\n",
    "freq = 0\n",
    "for e in range(args.num_epoch):\n",
    "    start_time_e = time.time()\n",
    "    print(f'Model Fitting: [{e+1}/{args.num_epoch}]')\n",
    "    for phase in ['train', 'valid']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        if phase == 'valid':\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_top1_acc = 0\n",
    "            val_top5_acc = 0\n",
    "            val_top10_acc = 0\n",
    "        for src, trg, king_id in tqdm(dataloader_dict[phase]):\n",
    "            # Sourcen, Target sentence setting\n",
    "            label_sequences = trg.to(device, non_blocking=True)\n",
    "            input_sequences = src.to(device, non_blocking=True)\n",
    "            king_id = king_id.to(device, non_blocking=True)\n",
    "\n",
    "            non_pad = label_sequences != args.pad_idx\n",
    "            trg_sequences_target = label_sequences[non_pad].contiguous().view(-1)\n",
    "\n",
    "            if args.model_setting == 'transformer':\n",
    "                # Target Masking\n",
    "                tgt_mask = model.generate_square_subsequent_mask(label_sequences.size(1))\n",
    "                tgt_mask = tgt_mask.to(device, non_blocking=True)\n",
    "                tgt_mask = tgt_mask.transpose(0, 1)\n",
    "\n",
    "            # Optimizer setting\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Model / Calculate loss\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                if args.model_setting == 'transformer':\n",
    "                    predicted = model(input_sequences, label_sequences, king_id, tgt_mask, non_pad)\n",
    "                if args.model_setting == 'rnn':\n",
    "#                     teacher_forcing_ratio_ = teacher_forcing_ratio if phase=='train' else 0 # for RNN model\n",
    "                    teacher_forcing_ratio_ = 0.5\n",
    "                    input_sequences = input_sequences.transpose(0, 1)\n",
    "                    label_sequences = label_sequences.transpose(0, 1)\n",
    "                    predicted = model(input_sequences, label_sequences, king_id, \n",
    "                                      teacher_forcing_ratio=teacher_forcing_ratio_)\n",
    "                    predicted = predicted.view(-1, trg_vocab_num)\n",
    "                    label_sequences = label_sequences.contiguous().view(-1)\n",
    "                loss = criterion(predicted, label_sequences)\n",
    "                if phase == 'valid':\n",
    "                    val_loss += loss.item()\n",
    "                    top1_acc, top5_acc, top10_acc = accuracy(predicted, \n",
    "                                                             trg_sequences_target, \n",
    "                                                             topk=(1,5,10))\n",
    "                    val_top1_acc += top1_acc.item()\n",
    "                    val_top5_acc += top5_acc.item()\n",
    "                    val_top10_acc += top10_acc.item()\n",
    "            # If phase train, then backward loss and step optimizer and scheduler\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_train_loss_list.append(loss.item())\n",
    "\n",
    "                # Print loss value only training\n",
    "                freq += 1\n",
    "                if freq == args.print_freq:\n",
    "                    total_loss = loss.item()\n",
    "                    top1_acc, top5_acc, top10_acc = accuracy(predicted, \n",
    "                                                             trg_sequences_target, \n",
    "                                                             topk=(1,5,10))\n",
    "                    print(\"[Epoch:%d] val_loss:%5.3f | top1_acc:%5.2f | top5_acc:%5.2f | spend_time:%5.2fmin\"\n",
    "                            % (e+1, total_loss, top1_acc, top5_acc, (time.time() - start_time_e) / 60))\n",
    "                    freq = 0\n",
    "\n",
    "        # Finishing iteration\n",
    "        if phase == 'valid':\n",
    "            val_loss /= len(dataloader_dict['valid'])\n",
    "            val_top1_acc /= len(dataloader_dict['valid'])\n",
    "            val_top5_acc /= len(dataloader_dict['valid'])\n",
    "            val_top10_acc /= len(dataloader_dict['valid'])\n",
    "            total_test_loss_list.append(val_loss)\n",
    "            print(\"[Epoch:%d] val_loss:%5.3f | top1_acc:%5.2f | top5_acc:%5.2f | top5_acc:%5.2f | spend_time:%5.2fmin\"\n",
    "                    % (e+1, total_loss, val_top1_acc, val_top5_acc, val_top10_acc, (time.time() - start_time_e) / 60))\n",
    "            if not best_val_loss or val_loss > best_val_loss:\n",
    "                print(\"[!] saving model...\")\n",
    "                if not os.path.exists(args.save_path):\n",
    "                    os.mkdir(args.save_path)\n",
    "                torch.save(model.state_dict(), \n",
    "                           os.path.join(args.save_path, f'nmt_model_{args.resume}.pt'))\n",
    "                best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
